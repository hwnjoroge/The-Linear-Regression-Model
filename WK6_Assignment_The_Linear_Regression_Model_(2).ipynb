{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WK6 : Assignment_The_Linear_Regression_Model (2).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9znpyK4UbceY"
      },
      "source": [
        "# Project Notebook: The Linear Regression Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JtBkHv4bdbL"
      },
      "source": [
        "## 1. Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Q7QTPAhbjGj"
      },
      "source": [
        "We started by building intuition for model based learning, explored how the linear regression model worked, understood how the two different approaches to model fitting worked, and some techniques for cleaning, transforming, and selecting features. In this project, you can practice what you learned by exploring ways to improve the models we built.\n",
        "\n",
        "You'll work with housing data for the city of Ames, Iowa, United States from 2006 to 2010. You can also read about the different columns in the data [here](https://s3.amazonaws.com/dq-content/307/data_description.txt).\n",
        "\n",
        "Let's start by setting up a pipeline of functions that will let us quickly iterate on different models.\n",
        "\n",
        "**Tasks**\n",
        "\n",
        "1. Import pandas, matplotlib, and numpy into the environment. Import the classes you need from scikit-learn as well.\n",
        "2. Read `AmesHousing.tsv` () into a pandas data frame.\n",
        "3. For the following functions, we recommend creating them in the first few cells in the notebook. This way, you can add cells to the end of the notebook to do experiments and update the functions in these cells.\n",
        "* Create a function named `transform_features()` that, for now, just returns the train data frame.\n",
        "* Create a function named `select_features()` that, for now, just returns the Gr Liv Area and SalePrice columns from the train data frame.\n",
        "* Create a function named `train_and_test()` that, for now:\n",
        "\n",
        "1. Selects the first 1460 rows from from data and assign to train.\n",
        "2. Selects the remaining rows from data and assign to test.\n",
        "3. Trains a model using all numerical columns except the SalePrice column (the target column) from the data frame returned from `select_features()`\n",
        "4. Tests the model on the test set and returns the `RMSE` value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTUjKMIObXHq"
      },
      "source": [
        "# import pandas and prerequisite classes\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import seaborn as sns\n"
      ],
      "execution_count": 175,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZ6Sh4m1bjXP"
      },
      "source": [
        "#Read AmesHousing.tsv () into a pandas data frame.\n",
        "data = pd.read_csv('https://bit.ly/3boZCX4', delimiter=\"\\t\")"
      ],
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 308
        },
        "id": "PtUc6kT4bwZ-",
        "outputId": "4aed8360-635e-47d0-bd1c-ac10626ac84a"
      },
      "source": [
        "data.head(5)"
      ],
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Order</th>\n",
              "      <th>PID</th>\n",
              "      <th>MS SubClass</th>\n",
              "      <th>MS Zoning</th>\n",
              "      <th>Lot Frontage</th>\n",
              "      <th>Lot Area</th>\n",
              "      <th>Street</th>\n",
              "      <th>Alley</th>\n",
              "      <th>Lot Shape</th>\n",
              "      <th>Land Contour</th>\n",
              "      <th>Utilities</th>\n",
              "      <th>Lot Config</th>\n",
              "      <th>Land Slope</th>\n",
              "      <th>Neighborhood</th>\n",
              "      <th>Condition 1</th>\n",
              "      <th>Condition 2</th>\n",
              "      <th>Bldg Type</th>\n",
              "      <th>House Style</th>\n",
              "      <th>Overall Qual</th>\n",
              "      <th>Overall Cond</th>\n",
              "      <th>Year Built</th>\n",
              "      <th>Year Remod/Add</th>\n",
              "      <th>Roof Style</th>\n",
              "      <th>Roof Matl</th>\n",
              "      <th>Exterior 1st</th>\n",
              "      <th>Exterior 2nd</th>\n",
              "      <th>Mas Vnr Type</th>\n",
              "      <th>Mas Vnr Area</th>\n",
              "      <th>Exter Qual</th>\n",
              "      <th>Exter Cond</th>\n",
              "      <th>Foundation</th>\n",
              "      <th>Bsmt Qual</th>\n",
              "      <th>Bsmt Cond</th>\n",
              "      <th>Bsmt Exposure</th>\n",
              "      <th>BsmtFin Type 1</th>\n",
              "      <th>BsmtFin SF 1</th>\n",
              "      <th>BsmtFin Type 2</th>\n",
              "      <th>BsmtFin SF 2</th>\n",
              "      <th>Bsmt Unf SF</th>\n",
              "      <th>Total Bsmt SF</th>\n",
              "      <th>...</th>\n",
              "      <th>Central Air</th>\n",
              "      <th>Electrical</th>\n",
              "      <th>1st Flr SF</th>\n",
              "      <th>2nd Flr SF</th>\n",
              "      <th>Low Qual Fin SF</th>\n",
              "      <th>Gr Liv Area</th>\n",
              "      <th>Bsmt Full Bath</th>\n",
              "      <th>Bsmt Half Bath</th>\n",
              "      <th>Full Bath</th>\n",
              "      <th>Half Bath</th>\n",
              "      <th>Bedroom AbvGr</th>\n",
              "      <th>Kitchen AbvGr</th>\n",
              "      <th>Kitchen Qual</th>\n",
              "      <th>TotRms AbvGrd</th>\n",
              "      <th>Functional</th>\n",
              "      <th>Fireplaces</th>\n",
              "      <th>Fireplace Qu</th>\n",
              "      <th>Garage Type</th>\n",
              "      <th>Garage Yr Blt</th>\n",
              "      <th>Garage Finish</th>\n",
              "      <th>Garage Cars</th>\n",
              "      <th>Garage Area</th>\n",
              "      <th>Garage Qual</th>\n",
              "      <th>Garage Cond</th>\n",
              "      <th>Paved Drive</th>\n",
              "      <th>Wood Deck SF</th>\n",
              "      <th>Open Porch SF</th>\n",
              "      <th>Enclosed Porch</th>\n",
              "      <th>3Ssn Porch</th>\n",
              "      <th>Screen Porch</th>\n",
              "      <th>Pool Area</th>\n",
              "      <th>Pool QC</th>\n",
              "      <th>Fence</th>\n",
              "      <th>Misc Feature</th>\n",
              "      <th>Misc Val</th>\n",
              "      <th>Mo Sold</th>\n",
              "      <th>Yr Sold</th>\n",
              "      <th>Sale Type</th>\n",
              "      <th>Sale Condition</th>\n",
              "      <th>SalePrice</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>526301100</td>\n",
              "      <td>20</td>\n",
              "      <td>RL</td>\n",
              "      <td>141.0</td>\n",
              "      <td>31770</td>\n",
              "      <td>Pave</td>\n",
              "      <td>NaN</td>\n",
              "      <td>IR1</td>\n",
              "      <td>Lvl</td>\n",
              "      <td>AllPub</td>\n",
              "      <td>Corner</td>\n",
              "      <td>Gtl</td>\n",
              "      <td>NAmes</td>\n",
              "      <td>Norm</td>\n",
              "      <td>Norm</td>\n",
              "      <td>1Fam</td>\n",
              "      <td>1Story</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>1960</td>\n",
              "      <td>1960</td>\n",
              "      <td>Hip</td>\n",
              "      <td>CompShg</td>\n",
              "      <td>BrkFace</td>\n",
              "      <td>Plywood</td>\n",
              "      <td>Stone</td>\n",
              "      <td>112.0</td>\n",
              "      <td>TA</td>\n",
              "      <td>TA</td>\n",
              "      <td>CBlock</td>\n",
              "      <td>TA</td>\n",
              "      <td>Gd</td>\n",
              "      <td>Gd</td>\n",
              "      <td>BLQ</td>\n",
              "      <td>639.0</td>\n",
              "      <td>Unf</td>\n",
              "      <td>0.0</td>\n",
              "      <td>441.0</td>\n",
              "      <td>1080.0</td>\n",
              "      <td>...</td>\n",
              "      <td>Y</td>\n",
              "      <td>SBrkr</td>\n",
              "      <td>1656</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1656</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>TA</td>\n",
              "      <td>7</td>\n",
              "      <td>Typ</td>\n",
              "      <td>2</td>\n",
              "      <td>Gd</td>\n",
              "      <td>Attchd</td>\n",
              "      <td>1960.0</td>\n",
              "      <td>Fin</td>\n",
              "      <td>2.0</td>\n",
              "      <td>528.0</td>\n",
              "      <td>TA</td>\n",
              "      <td>TA</td>\n",
              "      <td>P</td>\n",
              "      <td>210</td>\n",
              "      <td>62</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>2010</td>\n",
              "      <td>WD</td>\n",
              "      <td>Normal</td>\n",
              "      <td>215000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>526350040</td>\n",
              "      <td>20</td>\n",
              "      <td>RH</td>\n",
              "      <td>80.0</td>\n",
              "      <td>11622</td>\n",
              "      <td>Pave</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Reg</td>\n",
              "      <td>Lvl</td>\n",
              "      <td>AllPub</td>\n",
              "      <td>Inside</td>\n",
              "      <td>Gtl</td>\n",
              "      <td>NAmes</td>\n",
              "      <td>Feedr</td>\n",
              "      <td>Norm</td>\n",
              "      <td>1Fam</td>\n",
              "      <td>1Story</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>1961</td>\n",
              "      <td>1961</td>\n",
              "      <td>Gable</td>\n",
              "      <td>CompShg</td>\n",
              "      <td>VinylSd</td>\n",
              "      <td>VinylSd</td>\n",
              "      <td>None</td>\n",
              "      <td>0.0</td>\n",
              "      <td>TA</td>\n",
              "      <td>TA</td>\n",
              "      <td>CBlock</td>\n",
              "      <td>TA</td>\n",
              "      <td>TA</td>\n",
              "      <td>No</td>\n",
              "      <td>Rec</td>\n",
              "      <td>468.0</td>\n",
              "      <td>LwQ</td>\n",
              "      <td>144.0</td>\n",
              "      <td>270.0</td>\n",
              "      <td>882.0</td>\n",
              "      <td>...</td>\n",
              "      <td>Y</td>\n",
              "      <td>SBrkr</td>\n",
              "      <td>896</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>896</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>TA</td>\n",
              "      <td>5</td>\n",
              "      <td>Typ</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Attchd</td>\n",
              "      <td>1961.0</td>\n",
              "      <td>Unf</td>\n",
              "      <td>1.0</td>\n",
              "      <td>730.0</td>\n",
              "      <td>TA</td>\n",
              "      <td>TA</td>\n",
              "      <td>Y</td>\n",
              "      <td>140</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>120</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>MnPrv</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>2010</td>\n",
              "      <td>WD</td>\n",
              "      <td>Normal</td>\n",
              "      <td>105000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>526351010</td>\n",
              "      <td>20</td>\n",
              "      <td>RL</td>\n",
              "      <td>81.0</td>\n",
              "      <td>14267</td>\n",
              "      <td>Pave</td>\n",
              "      <td>NaN</td>\n",
              "      <td>IR1</td>\n",
              "      <td>Lvl</td>\n",
              "      <td>AllPub</td>\n",
              "      <td>Corner</td>\n",
              "      <td>Gtl</td>\n",
              "      <td>NAmes</td>\n",
              "      <td>Norm</td>\n",
              "      <td>Norm</td>\n",
              "      <td>1Fam</td>\n",
              "      <td>1Story</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1958</td>\n",
              "      <td>1958</td>\n",
              "      <td>Hip</td>\n",
              "      <td>CompShg</td>\n",
              "      <td>Wd Sdng</td>\n",
              "      <td>Wd Sdng</td>\n",
              "      <td>BrkFace</td>\n",
              "      <td>108.0</td>\n",
              "      <td>TA</td>\n",
              "      <td>TA</td>\n",
              "      <td>CBlock</td>\n",
              "      <td>TA</td>\n",
              "      <td>TA</td>\n",
              "      <td>No</td>\n",
              "      <td>ALQ</td>\n",
              "      <td>923.0</td>\n",
              "      <td>Unf</td>\n",
              "      <td>0.0</td>\n",
              "      <td>406.0</td>\n",
              "      <td>1329.0</td>\n",
              "      <td>...</td>\n",
              "      <td>Y</td>\n",
              "      <td>SBrkr</td>\n",
              "      <td>1329</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1329</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>Gd</td>\n",
              "      <td>6</td>\n",
              "      <td>Typ</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Attchd</td>\n",
              "      <td>1958.0</td>\n",
              "      <td>Unf</td>\n",
              "      <td>1.0</td>\n",
              "      <td>312.0</td>\n",
              "      <td>TA</td>\n",
              "      <td>TA</td>\n",
              "      <td>Y</td>\n",
              "      <td>393</td>\n",
              "      <td>36</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Gar2</td>\n",
              "      <td>12500</td>\n",
              "      <td>6</td>\n",
              "      <td>2010</td>\n",
              "      <td>WD</td>\n",
              "      <td>Normal</td>\n",
              "      <td>172000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>526353030</td>\n",
              "      <td>20</td>\n",
              "      <td>RL</td>\n",
              "      <td>93.0</td>\n",
              "      <td>11160</td>\n",
              "      <td>Pave</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Reg</td>\n",
              "      <td>Lvl</td>\n",
              "      <td>AllPub</td>\n",
              "      <td>Corner</td>\n",
              "      <td>Gtl</td>\n",
              "      <td>NAmes</td>\n",
              "      <td>Norm</td>\n",
              "      <td>Norm</td>\n",
              "      <td>1Fam</td>\n",
              "      <td>1Story</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>1968</td>\n",
              "      <td>1968</td>\n",
              "      <td>Hip</td>\n",
              "      <td>CompShg</td>\n",
              "      <td>BrkFace</td>\n",
              "      <td>BrkFace</td>\n",
              "      <td>None</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Gd</td>\n",
              "      <td>TA</td>\n",
              "      <td>CBlock</td>\n",
              "      <td>TA</td>\n",
              "      <td>TA</td>\n",
              "      <td>No</td>\n",
              "      <td>ALQ</td>\n",
              "      <td>1065.0</td>\n",
              "      <td>Unf</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1045.0</td>\n",
              "      <td>2110.0</td>\n",
              "      <td>...</td>\n",
              "      <td>Y</td>\n",
              "      <td>SBrkr</td>\n",
              "      <td>2110</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2110</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>Ex</td>\n",
              "      <td>8</td>\n",
              "      <td>Typ</td>\n",
              "      <td>2</td>\n",
              "      <td>TA</td>\n",
              "      <td>Attchd</td>\n",
              "      <td>1968.0</td>\n",
              "      <td>Fin</td>\n",
              "      <td>2.0</td>\n",
              "      <td>522.0</td>\n",
              "      <td>TA</td>\n",
              "      <td>TA</td>\n",
              "      <td>Y</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>2010</td>\n",
              "      <td>WD</td>\n",
              "      <td>Normal</td>\n",
              "      <td>244000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>527105010</td>\n",
              "      <td>60</td>\n",
              "      <td>RL</td>\n",
              "      <td>74.0</td>\n",
              "      <td>13830</td>\n",
              "      <td>Pave</td>\n",
              "      <td>NaN</td>\n",
              "      <td>IR1</td>\n",
              "      <td>Lvl</td>\n",
              "      <td>AllPub</td>\n",
              "      <td>Inside</td>\n",
              "      <td>Gtl</td>\n",
              "      <td>Gilbert</td>\n",
              "      <td>Norm</td>\n",
              "      <td>Norm</td>\n",
              "      <td>1Fam</td>\n",
              "      <td>2Story</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>1997</td>\n",
              "      <td>1998</td>\n",
              "      <td>Gable</td>\n",
              "      <td>CompShg</td>\n",
              "      <td>VinylSd</td>\n",
              "      <td>VinylSd</td>\n",
              "      <td>None</td>\n",
              "      <td>0.0</td>\n",
              "      <td>TA</td>\n",
              "      <td>TA</td>\n",
              "      <td>PConc</td>\n",
              "      <td>Gd</td>\n",
              "      <td>TA</td>\n",
              "      <td>No</td>\n",
              "      <td>GLQ</td>\n",
              "      <td>791.0</td>\n",
              "      <td>Unf</td>\n",
              "      <td>0.0</td>\n",
              "      <td>137.0</td>\n",
              "      <td>928.0</td>\n",
              "      <td>...</td>\n",
              "      <td>Y</td>\n",
              "      <td>SBrkr</td>\n",
              "      <td>928</td>\n",
              "      <td>701</td>\n",
              "      <td>0</td>\n",
              "      <td>1629</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>TA</td>\n",
              "      <td>6</td>\n",
              "      <td>Typ</td>\n",
              "      <td>1</td>\n",
              "      <td>TA</td>\n",
              "      <td>Attchd</td>\n",
              "      <td>1997.0</td>\n",
              "      <td>Fin</td>\n",
              "      <td>2.0</td>\n",
              "      <td>482.0</td>\n",
              "      <td>TA</td>\n",
              "      <td>TA</td>\n",
              "      <td>Y</td>\n",
              "      <td>212</td>\n",
              "      <td>34</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>MnPrv</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>2010</td>\n",
              "      <td>WD</td>\n",
              "      <td>Normal</td>\n",
              "      <td>189900</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 82 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   Order        PID  MS SubClass  ... Sale Type  Sale Condition  SalePrice\n",
              "0      1  526301100           20  ...       WD           Normal     215000\n",
              "1      2  526350040           20  ...       WD           Normal     105000\n",
              "2      3  526351010           20  ...       WD           Normal     172000\n",
              "3      4  526353030           20  ...       WD           Normal     244000\n",
              "4      5  527105010           60  ...       WD           Normal     189900\n",
              "\n",
              "[5 rows x 82 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 177
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJa7yKq65ROl"
      },
      "source": [
        "\n",
        "\n",
        "**Uncomment to run**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DLTmuiD6bjg1",
        "outputId": "d9db3116-e716-46bd-b496-cd7b03b6b112"
      },
      "source": [
        "#Create a function named transform_features() that, for now, just returns the train data frame.\n",
        "def transform_features(data):\n",
        "    return data\n",
        "\n",
        "transform_features"
      ],
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function __main__.transform_features>"
            ]
          },
          "metadata": {},
          "execution_count": 178
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lT4sP_wHbjtl",
        "outputId": "d5dd0054-01a4-4210-84b1-172e9bdea3a0"
      },
      "source": [
        "#Create a function named select_features() that, for now, just returns the Gr Liv Area and SalePrice columns from the train data frame.\n",
        "def select_features(data):\n",
        "    return data[['Gr Liv Area', 'SalePrice']]\n",
        "\n",
        "select_features\n",
        "\n"
      ],
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function __main__.select_features>"
            ]
          },
          "metadata": {},
          "execution_count": 179
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-X5JXR6b5Qe"
      },
      "source": [
        "# Create a function named train_and_test() that, for now:\n",
        "#Trains a model using all numerical columns except the SalePrice column (the target column) from the data frame returned from select_features()\n",
        "# Tests the model on the test set and returns the RMSE value.\n",
        "\n",
        "def train_and_test(data):\n",
        "#Using select_dtypes to select only numerical columns \n",
        "    train= data[0:1460]\n",
        "    test =data[1460:]\n",
        "\n",
        "    train_numerical_columns = train.select_dtypes(include = ['float','integer'])\n",
        "    test_numerical_columns = test.select_dtypes(include = ['float', 'integer'])\n",
        "\n",
        "    # #dropping the null values in the train and test numerical cols\n",
        "    # train_numerical_columns = train_numerical_columns.dropna()\n",
        "    # test_numerical_columns = test_numerical_columns.dropna()\n",
        "\n",
        "  # Using pd.drop to drop the target column 'SalePrice' from the train and test data\n",
        "    train_features = train_numerical_columns.drop(['SalePrice'], axis=1)\n",
        "    test_features = test_numerical_columns.drop(['SalePrice'], axis=1)\n",
        "\n",
        "    \n",
        "# # Identifying the target column 'SalePrice' from the train and test data   \n",
        "    train_target = train_numerical_columns['SalePrice']\n",
        "    test_target = test_numerical_columns['SalePrice']\n",
        "\n",
        "# #Creating a linear regregersiion instanace\n",
        "\n",
        "    linear_model = LinearRegression()\n",
        "\n",
        "# # Training the model\n",
        "    linear_model.fit(train_features, train_target)\n",
        "\n",
        "\n",
        "# # Predicting using the test set\n",
        "    predictions = linear_model.predict(test_features)\n",
        "\n",
        "# # Calculating the RMSE   \n",
        "    mse = mean_squared_error(test_target, predictions)\n",
        "    rmse = np.sqrt(mse)\n",
        "    return rmse\n",
        "\n",
        "\n"
      ],
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DmHe8pXUbj4D",
        "outputId": "5a3d7214-b39c-4a4b-a98f-d284c85be64d"
      },
      "source": [
        "transform_data = transform_features(data)\n",
        "filtered_data = select_features(transform_data)\n",
        "rmse = train_and_test(filtered_data)\n",
        "rmse"
      ],
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "57088.25161263909"
            ]
          },
          "metadata": {},
          "execution_count": 181
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUyr-Mz0c-DG"
      },
      "source": [
        "## 2. Feature Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfxdsJbvc_jo"
      },
      "source": [
        "Let's now start removing features with many missing values, diving deeper into potential categorical features, and transforming text and numerical columns. Update `transform_features()` so that any column from the data frame with more than 25% (or another cutoff value) missing values is dropped. You also need to remove any columns that leak information about the sale (e.g. like the year the sale happened). In general, the goal of this function is to:\n",
        "\n",
        "* remove features that we don't want to use in the model, just based on the number of missing values or data leakage.\n",
        "* transform features into the proper format (numerical to categorical, scaling numerical, filling in missing values, etc).\n",
        "* create new features by combining other features.\n",
        "\n",
        "Next, you need to get more familiar with the remaining columns by reading the data documentation for each column, determining what transformations are necessary (if any), and more. As we mentioned earlier, succeeding in predictive modeling (and competitions like Kaggle) is highly dependent on the quality of features the model has. Libraries like scikit-learn have made it quick and easy to simply try and tweak many different models, but cleaning, selecting, and transforming features are still more of an art that requires a bit of human ingenuity.\n",
        "\n",
        "**Tasks**\n",
        "\n",
        "1. As we mentioned earlier, we recommend adding some cells to explore and experiment with different features (before rewriting these functions).\n",
        "\n",
        "2. The `transform_features()` function shouldn't modify the train data frame and instead return a new one entirely. This way, we can keep using train in the experimentation cells.\n",
        "\n",
        "3. Which columns contain less than 5% missing values?\n",
        "* For numerical columns that meet this criteria, let's fill in the missing values using the most popular value for that column.\n",
        "\n",
        "4. What new features can we create, that better capture the information in some of the features?\n",
        "* An example of this would be the `years_until_remod` feature we created in the last lesson.\n",
        "\n",
        "5. Which columns need to be dropped for other reasons?\n",
        "* Which columns aren't useful for machine learning?\n",
        "* Which columns leak data about the final sale?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwTR1vFNNTft"
      },
      "source": [
        "#### Updating the transform function with feature engineering to see how the model improves\n",
        "\n",
        "**Uncomment to run**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# #Read AmesHousing.tsv () into a pandas data frame.\n",
        "# data = pd.read_csv('https://bit.ly/3boZCX4', delimiter=\"\\t\")\n",
        "\n",
        "# def transform_features(data):\n",
        "\n",
        "#     # Which columns contain less than 5% missing values? ie ratio of missing values < 5%\n",
        "#     # percentage of missing values in each column ie # ratio of missing values = (Number of missing qty/ number of observations )*100\n",
        "#     # saving missing values columns in a variable named percentage_missing\n",
        "#     percentage_missing = data.isnull().sum()/len(data)*100\n",
        "\n",
        "#     # saving column names in a variable named all_cols\n",
        "#     all_cols = data.columns\n",
        "\n",
        "#     # Which columns contain less than 5% missing values?\n",
        "#     # Instanciating a new variable to store column names having missing values less than the threshold ie have missing values <5%\n",
        "#     below_5 = [ ]\n",
        "\n",
        "#     for i in range(data.columns.shape[0]):\n",
        "#         if percentage_missing[i]<=5: #setting the missing threshold at less than or equal to 5%\n",
        "#             below_5.append(all_cols[i])\n",
        "\n",
        "#     # creating a new dataframe using the above variables\n",
        "#     new_data = data[below_5]\n",
        "#     new_data.head()\n",
        "\n",
        "#     #For numerical columns that contain less than 5% missing values, fill their missing values using the most popular value for that column.\n",
        "#     ## Get the numerical columns from new_data df\n",
        "#     numerical_cols = new_data.columns[new_data.dtypes!='object']\n",
        "\n",
        "#     #function to fill missing values with the mode for the numerical columns in new_data df\n",
        "#     fill_mode = lambda col: col.fillna(col.mode())\n",
        "\n",
        "#     #apply the fill_mode function to fill the missing values\n",
        "#     new_data[numerical_cols]=new_data[numerical_cols].apply(fill_mode)\n",
        "\n",
        "\n",
        "#     #Creating new features that better capture the information in some of the features # An example of this would be the years_until_remod feature we created in the last lesson.\n",
        "#     #Feature: How old was the property when it was sold? insert a new column in new_data df with the value of the property age\n",
        "#     new_data['years_until_sold'] = new_data['Yr Sold'] - new_data['Year Built']\n",
        "\n",
        "#     #dropping rows with negative qty on 'years_until_sold' column\n",
        "#     new_data.drop(new_data[new_data['years_until_sold'] < 0].index, inplace = True)\n",
        "\n",
        "\n",
        "#     # Feature: How long after selling was the property remodelled?\n",
        "#     new_data['years_remod_after_sale'] = new_data['Yr Sold'] - new_data['Year Remod/Add']\n",
        "\n",
        "#     #dropping rows with a negative qty on 'years_remod_after_sale' column\n",
        "#     new_data.drop(new_data[new_data['years_remod_after_sale'] < 0].index, inplace = True)\n",
        "\n",
        "\n",
        "#     # Feature : Age of the property when it was remodelled?\n",
        "#     new_data['years_until_remoddeling'] = new_data['Year Remod/Add'] - new_data['Year Built']\n",
        "\n",
        "#     #dropping the rows with negative qty on 'years_until_remoddeling' column\n",
        "#     new_data.drop(new_data[new_data['years_until_remoddeling'] < 0].index, inplace = True)\n",
        "\n",
        "#     # Dropping columns which aren't useful for machine learning,and leak data about the final sale\n",
        "#     columns_to_drop = ['Order','PID','Year Built','Year Remod/Add','Yr Sold', 'Mo Sold', 'Sale Type'\n",
        "#                         , 'Sale Condition' ]\n",
        "\n",
        "#     new_data.drop(columns_to_drop, axis = 1, inplace = True)\n",
        "\n",
        "#     return new_data\n",
        "\n",
        "\n",
        "#  #Calling the select_features function that returns the Gr Liv Area and SalePrice columns from the train data frame.\n",
        "# def select_features(data):\n",
        "#     return data[['Gr Liv Area', 'SalePrice']]\n",
        "\n",
        "# select_features \n",
        "\n",
        "\n",
        "# # Calling the train_and_test function that trains the model using all numerical columns except the SalePrice column (the target column)\n",
        "# # from the data frame returned from select_features(), Tests the model on the test set and returns the RMSE value.\n",
        "\n",
        "# def train_and_test(data):\n",
        "# #Using select_dtypes to select only numerical columns \n",
        "#     train= data[0:1460]\n",
        "#     test =data[1460:]\n",
        "\n",
        "#     train_numerical_columns = train.select_dtypes(include = ['float','integer'])\n",
        "#     test_numerical_columns = test.select_dtypes(include = ['float', 'integer'])\n",
        "\n",
        "#     # #dropping the null values in the train and test numerical cols\n",
        "#     # train_numerical_columns = train_numerical_columns.dropna()\n",
        "#     # test_numerical_columns = test_numerical_columns.dropna()\n",
        "\n",
        "#   # Using pd.drop to drop the target column 'SalePrice' from the train and test data\n",
        "#     train_features = train_numerical_columns.drop(['SalePrice'], axis=1)\n",
        "#     test_features = test_numerical_columns.drop(['SalePrice'], axis=1)\n",
        "\n",
        "    \n",
        "# # # Identifying the target column 'SalePrice' from the train and test data   \n",
        "#     train_target = train_numerical_columns['SalePrice']\n",
        "#     test_target = test_numerical_columns['SalePrice']\n",
        "\n",
        "# # # #Creating a linear regression instance  \n",
        "\n",
        "#     linear_model = LinearRegression()\n",
        "\n",
        "# # # Training the model\n",
        "#     linear_model.fit(train_features, train_target)\n",
        "\n",
        "\n",
        "# # # Predicting using the test set\n",
        "#     predictions = linear_model.predict(test_features)\n",
        "\n",
        "# # # Calculating the RMSE   \n",
        "#     mse = mean_squared_error(test_target, predictions)\n",
        "#     rmse = np.sqrt(mse)\n",
        "#     return rmse\n",
        "\n",
        "# transform_data = transform_features(data)\n",
        "# filtered_data = select_features(transform_data)\n",
        "# rmse = train_and_test(filtered_data)\n",
        "# rmse\n",
        "\n"
      ],
      "metadata": {
        "id": "1xaHGR-n0elG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yakjLNuznOT"
      },
      "source": [
        "The RMSE score improved from 57088.25161263909 to 55284.62277814025\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KkU7eR2bQV5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4fXZPVfd4IY"
      },
      "source": [
        "## 3. Feature Selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnGMP3LYd8fZ"
      },
      "source": [
        "Now that we have cleaned and transformed a lot of the features in the data set, it's time to move on to feature selection for numerical features.\n",
        "\n",
        "**Tasks**\n",
        "\n",
        "1. Generate a correlation heatmap matrix of the numerical features in the training data set.\n",
        "* Which features correlate strongly with our target column, `SalePrice`?\n",
        "* Calculate the correlation coefficients for the columns that seem to correlate well with `SalePrice`. Because we have a pipeline in place, it's easy to try different features and see which features result in a better cross validation score.\n",
        "\n",
        "2. Which columns in the data frame should be converted to the categorical data type? All of the columns that can be categorized as nominal variables are candidates for being converted to categorical. Here are some other things you should think about:\n",
        "* If a categorical column has hundreds of unique values (or categories), should you keep it? When you dummy code this column, hundreds of columns will need to be added back to the data frame.\n",
        "* Which categorical columns have a few unique values but more than 95% of the values in the column belong to a specific category? This would be similar to a low variance numerical feature (no variability in the data for the model to capture).\n",
        "\n",
        "3. Which columns are currently numerical but need to be encoded as categorical instead (because the numbers don't have any semantic meaning)?\n",
        "\n",
        "4. What are some ways we can explore which categorical columns \"correlate\" well with `SalePrice`?\n",
        "\n",
        "5. Update the logic for the `select_features()` function. This function should take in the new, modified train and test data frames that were returned from `transform_features()`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcR5-9-cemHN"
      },
      "source": [
        "# # # # Your code goes here\n",
        "\n",
        "\n",
        "# #Read AmesHousing.tsv () into a pandas data frame.\n",
        "# data = pd.read_csv('https://bit.ly/3boZCX4', delimiter=\"\\t\")\n",
        "\n",
        "\n",
        "# # # Generate a correlation heatmap matrix of the numerical features in the training data set. \n",
        "# train= data[0:1460]\n",
        "# test =data[1460:]\n",
        "# train_numerical_df = train.select_dtypes(include = ['float','integer'])\n",
        "# train_numerical_df.corr()['SalePrice'].abs().sort_values(ascending = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Calculate the correlation coefficients for the columns that seem to correlate well with SalePrice\n",
        "# #create new variable for correlation coefficient values greater than 0.7 since a relationship between two variables is generally considered strong when their r value is larger than 0.7\n",
        "# coef_saleprice = train_numerical_df.corr()['SalePrice'].abs().sort_values(ascending = False)\n",
        "# high_coef = coef_saleprice[coef_saleprice > 0.5].index\n",
        "# sns.heatmap(train_numerical_df[high_coef].corr().abs())"
      ],
      "metadata": {
        "id": "I57jMD9g51xg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vIzJJZrC6cd"
      },
      "source": [
        "\n",
        "It looks like following columns are strongly related to each other.\n",
        "\n",
        "Total Bsmt SF and 1st Flr SF\n",
        "TotRms AbvGrd and Gr Liv Area\n",
        "Garage Cars and Garage Area\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Below are the columns with categorical values /categorical data type? \n",
        "# # All of the columns that can be categorized as nominal variables are candidates for being converted to categorical. \n",
        "\n",
        "# train_categorical_cols = train.columns[train.dtypes=='object'].\n",
        "# train_categorical_cols"
      ],
      "metadata": {
        "id": "kl8WtXCX516_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WD-qPR7RZ4vu"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "It looks like following columns are categorical in nature.\n",
        "\n",
        "'MS Zoning', 'Street', 'Alley', 'Lot Shape', 'Land Contour',\n",
        "       'Utilities', 'Lot Config', 'Land Slope', 'Neighborhood', 'Condition 1',\n",
        "       'Condition 2', 'Bldg Type', 'House Style', 'Roof Style', 'Roof Matl',\n",
        "       'Exterior 1st', 'Exterior 2nd', 'Mas Vnr Type', 'Exter Qual',\n",
        "       'Exter Cond', 'Foundation', 'Bsmt Qual', 'Bsmt Cond', 'Bsmt Exposure',\n",
        "       'BsmtFin Type 1', 'BsmtFin Type 2', 'Heating', 'Heating QC',\n",
        "       'Central Air', 'Electrical', 'Kitchen Qual', 'Functional',\n",
        "       'Fireplace Qu', 'Garage Type', 'Garage Finish', 'Garage Qual',\n",
        "       'Garage Cond', 'Paved Drive', 'Pool QC', 'Fence', 'Misc Feature',\n",
        "       'Sale Type', 'Sale Condition\n",
        "\n",
        "\n",
        "We shall drop columns that  have more than 10 unique values before dummy coding the rest of the columns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# #checking the added columns after encoding\n",
        "# print(transform_data.shape)"
      ],
      "metadata": {
        "id": "G_DrHNaYisAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPO1Yb62jqmb"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "the dataframe transform_data now has 492 columns after adding the dummy columns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Update the logic for the select_features() function. \n",
        "# This function should take in the new, modified train and test data frames that were returned from transform_features()."
      ],
      "metadata": {
        "id": "RKrlAeKJisEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oj1e_gXRlaJU"
      },
      "source": [
        "#### Updating the select function with feature selection to see how the model improves\n",
        "\n",
        "**Uncomment to run**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " #Calling the select_features function and updating it with feature engineering done.\n",
        "\n",
        "def select_features(data):\n",
        "    train_numerical_df = train.select_dtypes(include = ['float','integer'])\n",
        "    train_numerical_df.corr()['SalePrice'].abs().sort_values(ascending = False)\n",
        "    coef_saleprice = train_numerical_df.corr()['SalePrice'].abs().sort_values(ascending = False)\n",
        "    high_coef = coef_saleprice[coef_saleprice > 0.5].index\n",
        "\n",
        "    train_categorical_cols = train.columns[train.dtypes=='object']\n",
        "    train_categorical_cols\n",
        "\n",
        "# testing which categorical columns have <10 unique values.\n",
        "    transform_categorical_cols = []\n",
        "    for col in train_categorical_cols:\n",
        "        if col in transform_data.columns:\n",
        "            transform_categorical_cols.append(col)\n",
        "\n",
        "# Geting the unique values in each categorical column\n",
        "    uniqueness_counts = transform_data[transform_categorical_cols].apply(lambda col: len(col.value_counts())).sort_values()\n",
        "\n",
        "# Seleting all the categorical columns with a count >10 unique values and assignning them a variable to be used in dropping\n",
        "    drop_nonuniq_cols = uniqueness_counts[uniqueness_counts > 10].index\n",
        "\n",
        "#updating the transform_data df after dropping these columns\n",
        "    transform_data = transform_data.drop(drop_nonuniq_cols, axis=1)\n",
        " \n",
        "    text_cols = transform_data.select_dtypes(include=['object'])\n",
        "    for col in text_cols:\n",
        "      transform_data[col] = transform_data[col].astype('category')\n",
        "    \n",
        "# Create dummy columns and adding them back to the transform_data df\n",
        "    transform_data = pd.concat([\n",
        "        transform_data, \n",
        "        pd.get_dummies(transform_data.select_dtypes(include=['category']))\n",
        "    ], axis=1)\n",
        "\n",
        "    \n",
        "    return data"
      ],
      "metadata": {
        "id": "c7wDIGa5pXaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlvUyZrjeo97"
      },
      "source": [
        "## 4. Train and Test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYllZ0xBetTh"
      },
      "source": [
        "Now for the final part of the pipeline, training and testing. When iterating on different features, using simple validation is a good idea. Let's add a parameter named `k` that controls the type of cross validation that occurs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LmYg5JWeuGt"
      },
      "source": [
        "**Tasks**\n",
        "\n",
        "1. The optional `k` parameter should accept integer values, with a default value of `0`.\n",
        "\n",
        "2. When `k` equals `0`, perform holdout validation (what we already implemented):\n",
        "\n",
        "* Select the first `1460` rows and assign to `train`.\n",
        "* Select the remaining rows and assign to test.\n",
        "* Train on `train` and `test` on test.\n",
        "* Compute the `RMSE` and return.\n",
        "\n",
        "3. When k equals 1, perform simple cross validation:\n",
        "\n",
        "* Shuffle the ordering of the rows in the data frame.\n",
        "* Select the first 1460 rows and assign to `fold_one`.\n",
        "* Select the remaining rows and assign to `fold_two`.\n",
        "* Train on `fold_one` and test on `fold_two`.\n",
        "* Train on `fold_two` and test on `fold_one`.\n",
        "* Compute the average RMSE and return.\n",
        "\n",
        "4. When `k` is greater than `0`, implement k-fold cross validation using `k` folds:\n",
        "\n",
        "* Perform `k-fold` cross validation using k folds.\n",
        "* Calculate the average `RMSE` value and return this value."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cD6kbGjz9t0"
      },
      "source": [
        "#### Updating the train and test function with cross validation\n",
        "\n",
        "**Uncomment to run**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def train_and_test(data, k=0):\n",
        "\n",
        "# # initialize count to 0\n",
        "#   if k == 0:\n",
        "#     train= data[0:1460]\n",
        "#     test =data[1460:]\n",
        "# #Using select_dtypes to select only numerical columns \n",
        "\n",
        "#     train_numerical_columns = train.select_dtypes(include = ['float','integer'])\n",
        "#     test_numerical_columns = test.select_dtypes(include = ['float', 'integer'])\n",
        "\n",
        "#   # Using pd.drop to drop the target column 'SalePrice' from the train and test data\n",
        "#     train_features = train_numerical_columns.drop(['SalePrice'], axis=1)\n",
        "#     test_features = test_numerical_columns.drop(['SalePrice'], axis=1)\n",
        "\n",
        "# # # Identifying the target column 'SalePrice' from the train and test data   \n",
        "#     train_target = train_numerical_columns['SalePrice']\n",
        "#     test_target = test_numerical_columns['SalePrice']\n",
        "\n",
        "# #  #Creating a linear regression instance  \n",
        "\n",
        "#     linear_model = LinearRegression()\n",
        "\n",
        "# # # Training the model\n",
        "#     linear_model.fit(train_features, train_target)\n",
        "\n",
        "\n",
        "# # # Predicting using the test set\n",
        "#     predictions = linear_model.predict(test_features)\n",
        "\n",
        "# # # Calculating the RMSE   \n",
        "#     mse = mean_squared_error(test_target, predictions)\n",
        "#     rmse = np.sqrt(mse)\n",
        "#     return rmse \n",
        "\n",
        "\n",
        "# # When k equals 1, perform simple cross validation:\n",
        "#   if k == 1:\n",
        "#         # shuffling all rows in the df\n",
        "#         shuffled_df = data.sample(frac=1, )\n",
        "#         train = data[:1460]\n",
        "#         test = data[1460:]\n",
        "        \n",
        "\n",
        "#         train_numerical_columns = train.select_dtypes(include = ['float','integer'])\n",
        "#         test_numerical_columns = test.select_dtypes(include = ['float', 'integer'])\n",
        "\n",
        "#       # Using pd.drop to drop the target column 'SalePrice' from the train and test data\n",
        "#         train_features = train_numerical_columns.drop(['SalePrice'], axis=1)\n",
        "#         test_features = test_numerical_columns.drop(['SalePrice'], axis=1)\n",
        "\n",
        "#     # # Identifying the target column 'SalePrice' from the train and test data   \n",
        "#         train_target = train_numerical_columns['SalePrice']\n",
        "#         test_target = test_numerical_columns['SalePrice']\n",
        "\n",
        "#       # #Creating a linear regression instance  \n",
        "#         linear_model = LinearRegression()  \n",
        "#         linear_model.fit(train_features, train_target)\n",
        "#         predictions_one = linear_model.predict(test_features)        \n",
        "        \n",
        "#         mse_one = mean_squared_error(test_target, predictions_one)\n",
        "#         rmse_one = np.sqrt(mse_one)\n",
        "        \n",
        "#         linear_model.fit(train_features, train_target)\n",
        "#         predictions_two = linear_model.predict(test_features)        \n",
        "       \n",
        "#         mse_two = mean_squared_error(test_target, predictions_two)\n",
        "#         rmse_two = np.sqrt(mse_two)\n",
        "        \n",
        "#         avg_rmse = np.mean([rmse_one, rmse_two])\n",
        "#         print(rmse_one)\n",
        "#         print(rmse_two)\n",
        "#         return avg_rmse\n",
        "\n",
        "#   else:\n",
        "#         kf = KFold(n_splits=k, shuffle=True)\n",
        "#         rmse_values = []\n",
        "#         for train_index, test_index, in kf.split(data):\n",
        "#             train = data.iloc[train_index]\n",
        "#             test = data.iloc[test_index]\n",
        "#             linear_model.fit(train_features, train_target)\n",
        "#             predictions = linear_model.predict(test_features)\n",
        "         \n",
        "\n",
        "#         mse = mean_squared_error(test_target, predictions)\n",
        "#         rmse = np.sqrt(mse)\n",
        "#         rmse_values.append(rmse)\n",
        "#         print(rmse_values)\n",
        "#         avg_rmse = np.mean(rmse_values)\n",
        "#         return avg_rmse\n",
        "          \n"
      ],
      "metadata": {
        "id": "b7Ns1npLisNH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7s3dHK4t7EES"
      },
      "source": [
        "## 5. Model Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvc1bCVhz4wh"
      },
      "source": [
        "#### Combining all the  functions with to see how our model has improved."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Read AmesHousing.tsv () into a pandas data frame.\n",
        "data = pd.read_csv('https://bit.ly/3boZCX4', delimiter=\"\\t\")"
      ],
      "metadata": {
        "id": "vsB6aV220eG8"
      },
      "execution_count": 200,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ca7UFYbR0jdu"
      },
      "source": [
        "#####Transform_features function"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_features(data):\n",
        "\n",
        "    # Which columns contain less than 5% missing values? ie ratio of missing values < 5%\n",
        "    # percentage of missing values in each column ie # ratio of missing values = (Number of missing qty/ number of observations )*100\n",
        "    # saving missing values columns in a variable named percentage_missing\n",
        "    percentage_missing = data.isnull().sum()/len(data)*100\n",
        "\n",
        "    # saving column names in a variable named all_cols\n",
        "    all_cols = data.columns\n",
        "\n",
        "    # Which columns contain less than 5% missing values?\n",
        "    # Instanciating a new variable to store column names having missing values less than the threshold ie have missing values <5%\n",
        "    below_5 = [ ]\n",
        "\n",
        "    for i in range(data.columns.shape[0]):\n",
        "        if percentage_missing[i]<=5: #setting the missing threshold at less than or equal to 5%\n",
        "            below_5.append(all_cols[i])\n",
        "\n",
        "    # creating a new dataframe using the above variables\n",
        "    new_data = data[below_5]\n",
        "    new_data.head()\n",
        "\n",
        "    #For numerical columns that contain less than 5% missing values, fill their missing values using the most popular value for that column.\n",
        "    ## Get the numerical columns from new_data df\n",
        "    numerical_cols = new_data.columns[new_data.dtypes!='object']\n",
        "\n",
        "    #function to fill missing values with the mode for the numerical columns in new_data df\n",
        "    fill_mode = lambda col: col.fillna(col.mode())\n",
        "\n",
        "    #apply the fill_mode function to fill the missing values\n",
        "    new_data[numerical_cols]=new_data[numerical_cols].apply(fill_mode)\n",
        "\n",
        "\n",
        "    #Creating new features that better capture the information in some of the features # An example of this would be the years_until_remod feature we created in the last lesson.\n",
        "    #Feature: How old was the property when it was sold? insert a new column in new_data df with the value of the property age\n",
        "    new_data['years_until_sold'] = new_data['Yr Sold'] - new_data['Year Built']\n",
        "\n",
        "    #dropping rows with negative qty on 'years_until_sold' column\n",
        "    new_data.drop(new_data[new_data['years_until_sold'] < 0].index, inplace = True)\n",
        "\n",
        "\n",
        "    # Feature: How long after selling was the property remodelled?\n",
        "    new_data['years_remod_after_sale'] = new_data['Yr Sold'] - new_data['Year Remod/Add']\n",
        "\n",
        "    #dropping rows with a negative qty on 'years_remod_after_sale' column\n",
        "    new_data.drop(new_data[new_data['years_remod_after_sale'] < 0].index, inplace = True)\n",
        "\n",
        "\n",
        "    # Feature : Age of the property when it was remodelled?\n",
        "    new_data['years_until_remoddeling'] = new_data['Year Remod/Add'] - new_data['Year Built']\n",
        "\n",
        "    #dropping the rows with negative qty on 'years_until_remoddeling' column\n",
        "    new_data.drop(new_data[new_data['years_until_remoddeling'] < 0].index, inplace = True)\n",
        "\n",
        "    # Dropping columns which aren't useful for machine learning,and leak data about the final sale\n",
        "    columns_to_drop = ['Order','PID','Year Built','Year Remod/Add','Yr Sold', 'Mo Sold', 'Sale Type'\n",
        "                        , 'Sale Condition' ]\n",
        "\n",
        "    new_data.drop(columns_to_drop, axis = 1, inplace = True)\n",
        "\n",
        "    return new_data  \n",
        "\n",
        " "
      ],
      "metadata": {
        "id": "ojpHaEQzqCM0"
      },
      "execution_count": 201,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gISoAAG203dd"
      },
      "source": [
        "#####Select_features function"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Calling the select_features function and updating it with feature engineering done.\n",
        "\n",
        "def select_features(new_data):\n",
        "    train_numerical_df = train.select_dtypes(include = ['float','integer'])\n",
        "    train_numerical_df.corr()['SalePrice'].abs().sort_values(ascending = False)\n",
        "    coef_saleprice = train_numerical_df.corr()['SalePrice'].abs().sort_values(ascending = False)\n",
        "    high_coef = coef_saleprice[coef_saleprice > 0.5].index\n",
        "\n",
        "    train_categorical_cols = train.columns[train.dtypes=='object']\n",
        "    train_categorical_cols\n",
        "\n",
        "# testing which categorical columns have <10 unique values.\n",
        "    transform_categorical_cols = []\n",
        "    for col in train_categorical_cols:\n",
        "        # if col in transform_data.columns:\n",
        "            transform_categorical_cols.append(col)\n",
        "\n",
        "# Geting the unique values in each categorical column\n",
        "    uniqueness_counts = new_data[transform_categorical_cols].apply(lambda col: len(col.value_counts())).sort_values()\n",
        "\n",
        "# Seleting all the categorical columns with a count >10 unique values and assignning them a variable to be used in dropping\n",
        "    drop_nonuniq_cols = uniqueness_counts[uniqueness_counts > 10].index\n",
        "\n",
        "#updating the transform_data df after dropping these columns\n",
        "    transform_data = new_data.drop(drop_nonuniq_cols, axis=1)\n",
        " \n",
        "    text_cols = new_data.select_dtypes(include=['object'])\n",
        "    for col in text_cols:\n",
        "      new_data[col] = new_data[col].astype('category')\n",
        "    \n",
        "# Create dummy columns and adding them back to the transform_data df\n",
        "    new_data = pd.concat([\n",
        "        new_data, \n",
        "        pd.get_dummies(new_data.select_dtypes(include=['category']))\n",
        "    ], axis=1)\n",
        "\n",
        "    \n",
        "    return data"
      ],
      "metadata": {
        "id": "M30rUQ660VmY"
      },
      "execution_count": 202,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7isQzVFy0_dW"
      },
      "source": [
        "#####Train_test function"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_test(data, k=0):\n",
        "\n",
        "# initialize count to 0\n",
        "  if k == 0:\n",
        "    train= data[0:1460]\n",
        "    test =data[1460:]\n",
        "#Using select_dtypes to select only numerical columns \n",
        "\n",
        "    train_numerical_columns = train.select_dtypes(include = ['float','integer'])\n",
        "    test_numerical_columns = test.select_dtypes(include = ['float', 'integer'])\n",
        "\n",
        "  # Using pd.drop to drop the target column 'SalePrice' from the train and test data\n",
        "    train_features = train_numerical_columns.drop(['SalePrice'], axis=1)\n",
        "    test_features = test_numerical_columns.drop(['SalePrice'], axis=1)\n",
        "\n",
        "# # Identifying the target column 'SalePrice' from the train and test data   \n",
        "    train_target = train_numerical_columns['SalePrice']\n",
        "    test_target = test_numerical_columns['SalePrice']\n",
        "\n",
        "#  #Creating a linear regression instance  \n",
        "\n",
        "    linear_model = LinearRegression()\n",
        "\n",
        "# # Training the model\n",
        "    linear_model.fit(train_features, train_target)\n",
        "\n",
        "\n",
        "# # Predicting using the test set\n",
        "    predictions = linear_model.predict(test_features)\n",
        "\n",
        "# # Calculating the RMSE   \n",
        "    mse = mean_squared_error(test_target, predictions)\n",
        "    rmse = np.sqrt(mse)\n",
        "    return rmse \n",
        "\n",
        "\n",
        "# When k equals 1, perform simple cross validation:\n",
        "  if k == 1:\n",
        "        # shuffling all rows in the df\n",
        "        shuffled_df = data.sample(frac=1, )\n",
        "        train = data[:1460]\n",
        "        test = data[1460:]\n",
        "        \n",
        "\n",
        "        train_numerical_columns = train.select_dtypes(include = ['float','integer'])\n",
        "        test_numerical_columns = test.select_dtypes(include = ['float', 'integer'])\n",
        "\n",
        "      # Using pd.drop to drop the target column 'SalePrice' from the train and test data\n",
        "        train_features = train_numerical_columns.drop(['SalePrice'], axis=1)\n",
        "        test_features = test_numerical_columns.drop(['SalePrice'], axis=1)\n",
        "\n",
        "    # # Identifying the target column 'SalePrice' from the train and test data   \n",
        "        train_target = train_numerical_columns['SalePrice']\n",
        "        test_target = test_numerical_columns['SalePrice']\n",
        "\n",
        "      # #Creating a linear regression instance  \n",
        "        linear_model = LinearRegression()  \n",
        "        linear_model.fit(train_features, train_target)\n",
        "        predictions_one = linear_model.predict(test_features)        \n",
        "        \n",
        "        mse_one = mean_squared_error(test_target, predictions_one)\n",
        "        rmse_one = np.sqrt(mse_one)\n",
        "        \n",
        "        linear_model.fit(train_features, train_target)\n",
        "        predictions_two = linear_model.predict(test_features)        \n",
        "       \n",
        "        mse_two = mean_squared_error(test_target, predictions_two)\n",
        "        rmse_two = np.sqrt(mse_two)\n",
        "        \n",
        "        avg_rmse = np.mean([rmse_one, rmse_two])\n",
        "        print(rmse_one)\n",
        "        print(rmse_two)\n",
        "        return avg_rmse\n",
        "\n",
        "  else:\n",
        "        kf = KFold(n_splits=k, shuffle=True)\n",
        "        rmse_values = []\n",
        "        for train_index, test_index, in kf.split(data):\n",
        "            train = data.iloc[train_index]\n",
        "            test = data.iloc[test_index]\n",
        "            linear_model.fit(train_features, train_target)\n",
        "            predictions = linear_model.predict(test_features)\n",
        "         \n",
        "\n",
        "        mse = mean_squared_error(test_target, predictions)\n",
        "        rmse = np.sqrt(mse)\n",
        "        rmse_values.append(rmse)\n",
        "        print(rmse_values)\n",
        "        avg_rmse = np.mean(rmse_values)\n",
        "        return avg_rmse\n"
      ],
      "metadata": {
        "id": "ff-Zb5SuqCPm"
      },
      "execution_count": 203,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform_data = transform_features(data)\n",
        "filtered_data = select_features(transform_data)\n",
        "rmse = train_and_test(filtered_data, k=3)\n",
        "rmse"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 988
        },
        "id": "y-R2loFMqCSw",
        "outputId": "ea311aa6-fdb3-4726-98ff-a85780cace85"
      },
      "execution_count": 204,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py:3069: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  self[k1] = value[k2]\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py:4174: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  errors=errors,\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:43: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py:4174: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  errors=errors,\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:50: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py:4174: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  errors=errors,\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-204-1fa550a748e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtransform_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfiltered_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransform_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mrmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_and_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mrmse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-202-6342fef627a3>\u001b[0m in \u001b[0;36mselect_features\u001b[0;34m(new_data)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Geting the unique values in each categorical column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0muniqueness_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtransform_categorical_cols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# Seleting all the categorical columns with a count >10 unique values and assignning them a variable to be used in dropping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2910\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2911\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2912\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2914\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[0;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1252\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1254\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_read_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mraise_missing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1255\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[0;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1302\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m                 \u001b[0mnot_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{not_found} not in index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m             \u001b[0;31m# we skip the warning on Categorical\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"['Misc Feature', 'Sale Condition', 'Garage Type', 'Fireplace Qu', 'Fence', 'Pool QC', 'Alley', 'Garage Finish', 'Garage Qual', 'Sale Type', 'Garage Cond'] not in index\""
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBRXPk5wflpP"
      },
      "source": [
        "## 5. Next Steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JV5JSMQfp_o"
      },
      "source": [
        "That's it for the guided steps. Here's some potenial next steps that you can take:\n",
        "\n",
        "1. Continue iteration on feature engineering:\n",
        "* Research some other approaches to feature engineering online around housing data.\n",
        "* Visit the Kaggle kernels [page](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/kernels) page for this dataset to see approaches others took.\n",
        "\n",
        "2. Improve your feature selection:\n",
        "* Research ways of doing feature selection better with categorical columns (something we didn't cover in this particular course)."
      ]
    }
  ]
}